{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2662 images belonging to 2 classes.\n",
      "Found 203 images belonging to 2 classes.\n",
      "2659/8000 [========>.....................] - ETA: 21s - loss: 0.5605 - accuracy: 0.7093WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8000 batches). You may need to use the repeat() function when building your dataset.\n",
      "8000/8000 [==============================] - 12s 1ms/step - loss: 0.5601 - accuracy: 0.7096 - val_loss: 0.3276 - val_accuracy: 0.8000\n",
      "203/203 [==============================] - 1s 4ms/step\n",
      "test_labels\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[139  12]\n",
      " [ 48   4]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "# ilkleme\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adım 1 - Convolution\n",
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Adım 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# 2. convolution katmanı\n",
    "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adım 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Adım 4 - YSA\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# CNN ve resimler\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,shear_range = 0.2,zoom_range = 0.2,horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255,shear_range = 0.2,zoom_range = 0.2,horizontal_flip = True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('../datasource/training_set', target_size = (64, 64), batch_size = 1, class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('../datasource/test_set', target_size = (64, 64), batch_size = 1, class_mode = 'binary')\n",
    "\n",
    "classifier.fit(training_set, steps_per_epoch = 8000, epochs = 1, validation_data = test_set, validation_steps = 200)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "test_set.reset()\n",
    "pred=classifier.predict(test_set,verbose=1)\n",
    "#pred = list(map(round,pred))\n",
    "pred[pred > .5] = 1\n",
    "pred[pred <= .5] = 0\n",
    "\n",
    "\n",
    "#labels = (training_set.class_indices)\n",
    "\n",
    "test_labels = []\n",
    "\n",
    "for i in range(0,int(203)):\n",
    "    test_labels.extend(np.array(test_set[i][1]))\n",
    "    \n",
    "print('test_labels')\n",
    "print(test_labels)\n",
    "\n",
    "#labels = (training_set.class_indices)\n",
    "'''\n",
    "idx = []  \n",
    "for i in test_set:\n",
    "    ixx = (test_set.batch_index - 1) * test_set.batch_size\n",
    "    ixx = test_set.filenames[ixx : ixx + test_set.batch_size]\n",
    "    idx.append(ixx)\n",
    "    print(i)\n",
    "    print(idx)\n",
    "'''\n",
    "fileNames = test_set.filenames\n",
    "#abc = test_set.\n",
    "#print(idx)\n",
    "#test_labels = test_set.\n",
    "result = pd.DataFrame()\n",
    "result['filenames']= fileNames\n",
    "result['predictions'] = pred\n",
    "result['test'] = test_labels   \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cm = confusion_matrix(test_labels, pred)\n",
    "print (cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
